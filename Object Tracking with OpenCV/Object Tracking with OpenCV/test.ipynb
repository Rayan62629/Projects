{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] -p PROTOTXT -m MODEL [-c CONFIDENCE]\n",
      "ipykernel_launcher.py: error: the following arguments are required: -p/--prototxt, -m/--model\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abdul vaseem siddiqu\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:3534: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# Jupyter notebook code\n",
    "\n",
    "# Import necessary libraries\n",
    "from pyimagesearch.centroidtracker import CentroidTracker\n",
    "from imutils.video import VideoStream\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "# construct the argument parse and parse the arguments\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-p\", \"--prototxt\", required=True,\n",
    "                help=\"path to Caffe 'deploy' prototxt file\")\n",
    "ap.add_argument(\"-m\", \"--model\", required=True,\n",
    "                help=\"path to Caffe pre-trained model\")\n",
    "ap.add_argument(\"-c\", \"--confidence\", type=float, default=0.5,\n",
    "                help=\"minimum probability to filter weak detections\")\n",
    "args = vars(ap.parse_args())\n",
    "\n",
    "# initialize our centroid tracker and frame dimensions\n",
    "ct = CentroidTracker()\n",
    "(H, W) = (None, None)\n",
    "\n",
    "# load our serialized model from disk\n",
    "print(\"[INFO] loading model...\")\n",
    "net = cv2.dnn.readNetFromCaffe(args[\"prototxt\"], args[\"model\"])\n",
    "\n",
    "# initialize the video stream and allow the camera sensor to warmup\n",
    "print(\"[INFO] starting video stream...\")\n",
    "vs = VideoStream(src=0).start()\n",
    "time.sleep(2.0)\n",
    "\n",
    "# loop over the frames from the video stream\n",
    "while True:\n",
    "    # read the next frame from the video stream and resize it\n",
    "    frame = vs.read()\n",
    "    frame = imutils.resize(frame, width=400)\n",
    "\n",
    "    # if the frame dimensions are None, grab them\n",
    "    if W is None or H is None:\n",
    "        (H, W) = frame.shape[:2]\n",
    "\n",
    "    # construct a blob from the frame, pass it through the network,\n",
    "    # obtain our output predictions, and initialize the list of\n",
    "    # bounding box rectangles\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1.0, (W, H),\n",
    "                                (104.0, 177.0, 123.0))\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "    rects = []\n",
    "\n",
    "    # loop over the detections\n",
    "    for i in range(0, detections.shape[2]):\n",
    "        # filter out weak detections by ensuring the predicted\n",
    "        # probability is greater than a minimum threshold\n",
    "        if detections[0, 0, i, 2] > args[\"confidence\"]:\n",
    "            # compute the (x, y)-coordinates of the bounding box for\n",
    "            # the object, then update the bounding box rectangles list\n",
    "            box = detections[0, 0, i, 3:7] * np.array([W, H, W, H])\n",
    "            rects.append(box.astype(\"int\"))\n",
    "\n",
    "            # draw a bounding box surrounding the object so we can\n",
    "            # visualize it\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "            cv2.rectangle(frame, (startX, startY), (endX, endY),\n",
    "                          (0, 255, 0), 2)\n",
    "\n",
    "    # update our centroid tracker using the computed set of bounding\n",
    "    # box rectangles\n",
    "    objects = ct.update(rects)\n",
    "\n",
    "    # loop over the tracked objects\n",
    "    for (objectID, centroid) in objects.items():\n",
    "        # draw both the ID of the object and the centroid of the\n",
    "        # object on the output frame\n",
    "        text = \"ID {}\".format(objectID)\n",
    "        cv2.putText(frame, text, (centroid[0] - 10, centroid[1] - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        cv2.circle(frame, (centroid[0], centroid[1]), 4, (0, 255, 0), -1)\n",
    "\n",
    "    # show the output frame\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    # if the `q` key was pressed, break from the loop\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# do a bit of cleanup\n",
    "cv2.destroyAllWindows()\n",
    "vs.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading model...\n",
      "[INFO] starting video stream...\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from pyimagesearch.centroidtracker import CentroidTracker\n",
    "from imutils.video import VideoStream\n",
    "import numpy as np\n",
    "import imutils\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "# Manually set the values for prototxt, model, and confidence\n",
    "prototxt = \"./deploy.prototxt\"\n",
    "model = \"./res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "confidence = 0.5\n",
    "\n",
    "# initialize our centroid tracker and frame dimensions\n",
    "ct = CentroidTracker()\n",
    "(H, W) = (None, None)\n",
    "\n",
    "# load our serialized model from disk\n",
    "print(\"[INFO] loading model...\")\n",
    "net = cv2.dnn.readNetFromCaffe(prototxt, model)\n",
    "\n",
    "# initialize the video stream and allow the camera sensor to warmup\n",
    "print(\"[INFO] starting video stream...\")\n",
    "vs = VideoStream(src=0).start()\n",
    "time.sleep(2.0)\n",
    "\n",
    "# loop over the frames from the video stream\n",
    "while True:\n",
    "    # read the next frame from the video stream and resize it\n",
    "    frame = vs.read()\n",
    "    frame = imutils.resize(frame, width=400)\n",
    "\n",
    "    # if the frame dimensions are None, grab them\n",
    "    if W is None or H is None:\n",
    "        (H, W) = frame.shape[:2]\n",
    "\n",
    "    # construct a blob from the frame, pass it through the network,\n",
    "    # obtain our output predictions, and initialize the list of\n",
    "    # bounding box rectangles\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1.0, (W, H), (104.0, 177.0, 123.0))\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "    rects = []\n",
    "\n",
    "    # loop over the detections\n",
    "    for i in range(0, detections.shape[2]):\n",
    "        # filter out weak detections by ensuring the predicted\n",
    "        # probability is greater than a minimum threshold\n",
    "        if detections[0, 0, i, 2] > confidence:\n",
    "            # compute the (x, y)-coordinates of the bounding box for\n",
    "            # the object, then update the bounding box rectangles list\n",
    "            box = detections[0, 0, i, 3:7] * np.array([W, H, W, H])\n",
    "            rects.append(box.astype(\"int\"))\n",
    "\n",
    "            # draw a bounding box surrounding the object so we can\n",
    "            # visualize it\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "            cv2.rectangle(frame, (startX, startY), (endX, endY), (0, 255, 0), 2)\n",
    "\n",
    "    # update our centroid tracker using the computed set of bounding\n",
    "    # box rectangles\n",
    "    objects = ct.update(rects)\n",
    "\n",
    "    # loop over the tracked objects\n",
    "    for (objectID, centroid) in objects.items():\n",
    "        # draw both the ID of the object and the centroid of the\n",
    "        # object on the output frame\n",
    "        text = \"ID {}\".format(objectID)\n",
    "        cv2.putText(frame, text, (centroid[0] - 10, centroid[1] - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        cv2.circle(frame, (centroid[0], centroid[1]), 4, (0, 255, 0), -1)\n",
    "\n",
    "    # show the output frame\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    # if the `q` key was pressed, break from the loop\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# do a bit of cleanup\n",
    "cv2.destroyAllWindows()\n",
    "vs.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyimagesearch.centroidtracker import CentroidTracker\n",
    "import numpy as np\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "# Manually set the values for YOLO\n",
    "config_path = \"./yolov3.cfg\"\n",
    "weights_path = \"./yolov3.weights\"\n",
    "confidence_threshold = 0.5\n",
    "\n",
    "# Load YOLO\n",
    "net = cv2.dnn.readNetFromDarknet(config_path, weights_path)\n",
    "ln = net.getUnconnectedOutLayersNames()\n",
    "\n",
    "# initialize our centroid tracker and frame dimensions\n",
    "ct = CentroidTracker()\n",
    "(H, W) = (None, None)\n",
    "\n",
    "# initialize the video stream from a given video file\n",
    "video_path = \"./input_video.mp4\"\n",
    "vs = cv2.VideoCapture(video_path)\n",
    "\n",
    "# loop over the frames from the video stream\n",
    "while True:\n",
    "    # read the next frame from the video stream\n",
    "    ret, frame = vs.read()\n",
    "\n",
    "    # if the frame is not grabbed, we have reached the end of the video\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # resize the frame for processing\n",
    "    frame = cv2.resize(frame, (400, 300))\n",
    "\n",
    "    # if the frame dimensions are None, grab them\n",
    "    if W is None or H is None:\n",
    "        (H, W) = frame.shape[:2]\n",
    "\n",
    "    # Detect objects using YOLO\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward(ln)\n",
    "\n",
    "    # List to store bounding box rectangles\n",
    "    rects = []\n",
    "\n",
    "    # loop over the detections\n",
    "    for detection in detections:\n",
    "        for obj in detection:\n",
    "            scores = obj[5:]\n",
    "            classID = np.argmax(scores)\n",
    "            confidence = scores[classID]\n",
    "\n",
    "            if confidence > confidence_threshold:\n",
    "                # scale bounding box coordinates back relative to the size of the image\n",
    "                box = obj[0:4] * np.array([W, H, W, H])\n",
    "                (centerX, centerY, width, height) = box.astype(\"int\")\n",
    "\n",
    "                x = int(centerX - (width / 2))\n",
    "                y = int(centerY - (height / 2))\n",
    "\n",
    "                rects.append((x, y, int(width), int(height)))\n",
    "\n",
    "    # update our centroid tracker using the computed set of bounding box rectangles\n",
    "    objects = ct.update(rects)\n",
    "\n",
    "    # loop over the tracked objects\n",
    "    for (objectID, centroid) in objects.items():\n",
    "        # draw both the ID of the object and the centroid of the object on the output frame\n",
    "        text = \"ID {}\".format(objectID)\n",
    "        cv2.putText(frame, text, (centroid[0] - 10, centroid[1] - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        cv2.circle(frame, (centroid[0], centroid[1]), 4, (0, 255, 0), -1)\n",
    "\n",
    "    # show the output frame\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    # if the `q` key was pressed, break from the loop\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# release the video stream and close all windows\n",
    "vs.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyimagesearch.centroidtracker import CentroidTracker\n",
    "import numpy as np\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "# Manually set the values for YOLO (use YOLOv3-tiny for faster inference)\n",
    "config_path = \"./yolov3-tiny.cfg\"\n",
    "weights_path = \"./yolov3-tiny.weights\"\n",
    "confidence_threshold = 0.2\n",
    "\n",
    "# Load YOLO\n",
    "net = cv2.dnn.readNetFromDarknet(config_path, weights_path)\n",
    "ln = net.getUnconnectedOutLayersNames()\n",
    "\n",
    "# initialize our centroid tracker and frame dimensions\n",
    "ct = CentroidTracker()\n",
    "(H, W) = (None, None)\n",
    "\n",
    "# initialize the video stream from a given video file\n",
    "video_path = \"./input_video.mp4\"\n",
    "vs = cv2.VideoCapture(video_path)\n",
    "\n",
    "# loop over the frames from the video stream\n",
    "while True:\n",
    "    # read the next frame from the video stream\n",
    "    ret, frame = vs.read()\n",
    "\n",
    "    # if the frame is not grabbed, we have reached the end of the video\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # resize the frame for faster processing\n",
    "    frame = cv2.resize(frame, (400, 300))\n",
    "\n",
    "    # if the frame dimensions are None, grab them\n",
    "    if W is None or H is None:\n",
    "        (H, W) = frame.shape[:2]\n",
    "\n",
    "    # Detect objects using YOLO\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward(ln)\n",
    "\n",
    "    # List to store bounding box rectangles\n",
    "    rects = []\n",
    "\n",
    "    # loop over the detections\n",
    "    for detection in detections:\n",
    "        for obj in detection:\n",
    "            scores = obj[5:]\n",
    "            classID = np.argmax(scores)\n",
    "            confidence = scores[classID]\n",
    "\n",
    "            if confidence > confidence_threshold:\n",
    "                # scale bounding box coordinates back relative to the size of the image\n",
    "                box = obj[0:4] * np.array([W, H, W, H])\n",
    "                (x, y, width, height) = box.astype(\"int\")\n",
    "\n",
    "                # ensure the bounding box coordinates fall within the frame dimensions\n",
    "                x = max(0, x)\n",
    "                y = max(0, y)\n",
    "                width = min(W, width)\n",
    "                height = min(H, height)\n",
    "\n",
    "                rects.append((x, y, width, height))\n",
    "\n",
    "    # update our centroid tracker using the computed set of bounding box rectangles\n",
    "    objects = ct.update(rects)\n",
    "\n",
    "    # loop over the tracked objects\n",
    "    for (objectID, centroid) in objects.items():\n",
    "        # draw both the ID of the object and the centroid of the object on the output frame\n",
    "        text = \"ID {}\".format(objectID)\n",
    "        cv2.putText(frame, text, (centroid[0] - 10, centroid[1] - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        cv2.circle(frame, (centroid[0], centroid[1]), 4, (0, 255, 0), -1)\n",
    "\n",
    "    # show the output frame\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    # if the `q` key was pressed, break from the loop\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# release the video stream and close all windows\n",
    "vs.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\MITS MAIN\\MITS\\DataScience\\PROJECTS\\Simple-object-tracking-with-OpenCV-master\\Simple-object-tracking-with-OpenCV-master\\test.ipynb Cell 5\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/MITS%20MAIN/MITS/DataScience/PROJECTS/Simple-object-tracking-with-OpenCV-master/Simple-object-tracking-with-OpenCV-master/test.ipynb#W4sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m     cv2\u001b[39m.\u001b[39mputText(frame, text, (centroid[\u001b[39m0\u001b[39m] \u001b[39m-\u001b[39m \u001b[39m10\u001b[39m, centroid[\u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m \u001b[39m10\u001b[39m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/MITS%20MAIN/MITS/DataScience/PROJECTS/Simple-object-tracking-with-OpenCV-master/Simple-object-tracking-with-OpenCV-master/test.ipynb#W4sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m                 cv2\u001b[39m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[39m0.5\u001b[39m, (\u001b[39m0\u001b[39m, \u001b[39m255\u001b[39m, \u001b[39m0\u001b[39m), \u001b[39m2\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/MITS%20MAIN/MITS/DataScience/PROJECTS/Simple-object-tracking-with-OpenCV-master/Simple-object-tracking-with-OpenCV-master/test.ipynb#W4sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m     \u001b[39m# Draw a square frame around the object\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/MITS%20MAIN/MITS/DataScience/PROJECTS/Simple-object-tracking-with-OpenCV-master/Simple-object-tracking-with-OpenCV-master/test.ipynb#W4sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m     x, y, width, height \u001b[39m=\u001b[39m rects[objectID]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/MITS%20MAIN/MITS/DataScience/PROJECTS/Simple-object-tracking-with-OpenCV-master/Simple-object-tracking-with-OpenCV-master/test.ipynb#W4sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m     cv2\u001b[39m.\u001b[39mrectangle(frame, (x, y), (x \u001b[39m+\u001b[39m width, y \u001b[39m+\u001b[39m height), (\u001b[39m0\u001b[39m, \u001b[39m255\u001b[39m, \u001b[39m0\u001b[39m), \u001b[39m2\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/MITS%20MAIN/MITS/DataScience/PROJECTS/Simple-object-tracking-with-OpenCV-master/Simple-object-tracking-with-OpenCV-master/test.ipynb#W4sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m \u001b[39m# show the output frame\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from pyimagesearch.centroidtracker import CentroidTracker\n",
    "import numpy as np\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "# Manually set the values for YOLO (use YOLOv3-tiny for faster inference)\n",
    "config_path = \"./yolov3-tiny.cfg\"\n",
    "weights_path = \"./yolov3-tiny.weights\"\n",
    "confidence_threshold = 0.5\n",
    "\n",
    "# Load YOLO\n",
    "net = cv2.dnn.readNetFromDarknet(config_path, weights_path)\n",
    "ln = net.getUnconnectedOutLayersNames()\n",
    "\n",
    "# initialize our centroid tracker and frame dimensions\n",
    "ct = CentroidTracker()\n",
    "(H, W) = (None, None)\n",
    "\n",
    "# initialize the video stream from a given video file\n",
    "video_path = \"./input_video.mp4\"\n",
    "vs = cv2.VideoCapture(video_path)\n",
    "\n",
    "# loop over the frames from the video stream\n",
    "while True:\n",
    "    # read the next frame from the video stream\n",
    "    ret, frame = vs.read()\n",
    "\n",
    "    # if the frame is not grabbed, we have reached the end of the video\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # resize the frame for faster processing\n",
    "    frame = cv2.resize(frame, (400, 300))\n",
    "\n",
    "    # if the frame dimensions are None, grab them\n",
    "    if W is None or H is None:\n",
    "        (H, W) = frame.shape[:2]\n",
    "\n",
    "    # Detect objects using YOLO\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward(ln)\n",
    "\n",
    "    # List to store bounding box rectangles\n",
    "    rects = []\n",
    "\n",
    "    # loop over the detections\n",
    "    for detection in detections:\n",
    "        for obj in detection:\n",
    "            scores = obj[5:]\n",
    "            classID = np.argmax(scores)\n",
    "            confidence = scores[classID]\n",
    "\n",
    "            if confidence > confidence_threshold:\n",
    "                # scale bounding box coordinates back relative to the size of the image\n",
    "                box = obj[0:4] * np.array([W, H, W, H])\n",
    "                (x, y, width, height) = box.astype(\"int\")\n",
    "\n",
    "                # ensure the bounding box coordinates fall within the frame dimensions\n",
    "                x = max(0, x)\n",
    "                y = max(0, y)\n",
    "                width = min(W, width)\n",
    "                height = min(H, height)\n",
    "\n",
    "                rects.append((x, y, width, height))\n",
    "\n",
    "    # update our centroid tracker using the computed set of bounding box rectangles\n",
    "    objects = ct.update(rects)\n",
    "\n",
    "    # loop over the tracked objects\n",
    "    for (objectID, centroid) in objects.items():\n",
    "        # draw both the ID of the object and a square frame around the object on the output frame\n",
    "        text = \"ID {}\".format(objectID)\n",
    "        cv2.putText(frame, text, (centroid[0] - 10, centroid[1] - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "        # Draw a square frame around the object\n",
    "        x, y, width, height = rects[objectID]\n",
    "        cv2.rectangle(frame, (x, y), (x + width, y + height), (0, 255, 0), 2)\n",
    "\n",
    "    # show the output frame\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    # if the `q` key was pressed, break from the loop\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# release the video stream and close all windows\n",
    "vs.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
